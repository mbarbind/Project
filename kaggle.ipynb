{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.layers.convolutional import *\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the input tensor to shape [samples, rows, columns, channels]\n",
    "#x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
    "#x_train /= 255\n",
    "x_test /= 255\n",
    "#for converting catagorial data labels to numerical labels one hot encoding is used\n",
    "num_classes = 10\n",
    "#y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning hyperparameters\n",
    "img_dim = 28\n",
    "num_channels = 1\n",
    "\n",
    "#for first convolutional layer\n",
    "num_filters1 = 32\n",
    "kernel_size1 = 3\n",
    "\n",
    "#for second convolutional layer\n",
    "num_filters2 = 32\n",
    "kernel_size2 = 3\n",
    "\n",
    "#for first maxpooling layer\n",
    "pool_size1 = 2\n",
    "\n",
    "#for third convolutional layer\n",
    "num_filters3 = 64\n",
    "kernel_size3 = 3\n",
    "\n",
    "#for second maxpooling layer\n",
    "pool_size2 = 2 \n",
    "\n",
    "#for first fully connected layer\n",
    "dense_units1 = 1024\n",
    "\n",
    "#for second fully connected layer (since its final layer hence dense_units = num_classes)\n",
    "dense_units2 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    #add input_shape argument in case of the first layer which defines (row, col, channels)\n",
    "    model.add(Conv2D(num_filters1, (kernel_size1, kernel_size1), input_shape=(img_dim, img_dim, num_channels), activation = 'relu'))\n",
    "    #here axis = -1 which means channel_last convention its function: Normalize the activations of the previous layer at each batch, i.e. applies a \n",
    "    #transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "    model.add(BatchNormalization(axis = -1))\n",
    "    #if strides not defined then it is set to (1, 1) in case of Conv2D\n",
    "    model.add(Conv2D(num_filters2, (kernel_size2, kernel_size2), activation = 'relu'))\n",
    "    model.add(BatchNormalization(axis = -1))\n",
    "    #if strides not defined then it is set by default to pool_size\n",
    "    model.add(MaxPooling2D(pool_size = (pool_size1, pool_size1)))\n",
    "    model.add(Conv2D(num_filters3, (kernel_size3, kernel_size3), activation = 'relu'))\n",
    "    model.add(BatchNormalization(axis = -1))\n",
    "    model.add(MaxPooling2D(pool_size = (pool_size2, pool_size2)))  \n",
    "    model.add(Flatten())\n",
    "    #by default it used biases and baises initialised to 0\n",
    "    model.add(Dense(dense_units1, activation = 'relu'))\n",
    "    #dropout introduced to avoid the overfitting\n",
    "    model.add(Dropout(0.2))\n",
    "    #softmax activation produces probability over classes\n",
    "    model.add(Dense(dense_units2, activation = 'softmax'))\n",
    "    \n",
    "    #now compile model\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "(42000, 28, 28, 1)\n",
      "Epoch 1/20\n",
      " - 236s - loss: 0.2005 - acc: 0.9516\n",
      "Epoch 2/20\n",
      " - 255s - loss: 0.0358 - acc: 0.9886\n",
      "Epoch 3/20\n",
      " - 257s - loss: 0.0267 - acc: 0.9916\n",
      "Epoch 4/20\n",
      " - 259s - loss: 0.0186 - acc: 0.9939\n",
      "Epoch 5/20\n",
      " - 255s - loss: 0.0176 - acc: 0.9945\n",
      "Epoch 6/20\n",
      " - 254s - loss: 0.0148 - acc: 0.9954\n",
      "Epoch 7/20\n",
      " - 256s - loss: 0.0151 - acc: 0.9947\n",
      "Epoch 8/20\n",
      " - 256s - loss: 0.0119 - acc: 0.9962\n",
      "Epoch 9/20\n",
      " - 257s - loss: 0.0143 - acc: 0.9955\n",
      "Epoch 10/20\n",
      " - 256s - loss: 0.0156 - acc: 0.9956\n",
      "Epoch 11/20\n",
      " - 256s - loss: 0.0107 - acc: 0.9966\n",
      "Epoch 12/20\n",
      " - 258s - loss: 0.0100 - acc: 0.9973\n",
      "Epoch 13/20\n",
      " - 257s - loss: 0.0150 - acc: 0.9955\n",
      "Epoch 14/20\n",
      " - 259s - loss: 0.0144 - acc: 0.9961\n",
      "Epoch 15/20\n",
      " - 260s - loss: 0.0111 - acc: 0.9970\n",
      "Epoch 16/20\n",
      " - 258s - loss: 0.0109 - acc: 0.9971\n",
      "Epoch 17/20\n",
      " - 259s - loss: 0.0086 - acc: 0.9978\n",
      "Epoch 18/20\n",
      " - 261s - loss: 0.0050 - acc: 0.9985\n",
      "Epoch 19/20\n",
      " - 261s - loss: 0.0066 - acc: 0.9981\n",
      "Epoch 20/20\n",
      " - 260s - loss: 0.0114 - acc: 0.9970\n",
      "Baseline Error: 0.38%\n"
     ]
    }
   ],
   "source": [
    "#build model by calling create_model function\n",
    "model = create_model()\n",
    "#fit the model \n",
    "train = pd.read_csv(\"/home/mayur/Deep_learning_and_Project/tensorflow/handwritten_digits/train.csv\")\n",
    "x_train = train.iloc[:, 1:].values\n",
    "y_train = train.iloc[:, 0:1].values\n",
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0], img_dim, img_dim, num_channels).astype('float32')\n",
    "print(x_train.shape)\n",
    "x_train /= 255\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "model.fit(x = x_train, y = y_train, epochs = 20, batch_size = 200, verbose = 2)\n",
    "#final evaluation of model\n",
    "#evaluation of the model done on the test data\n",
    "#verbose 0 mean silent processing\n",
    "scores = model.evaluate(x = x_test, y = y_test, verbose = 0)\n",
    "\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.07343888e-30   6.01892165e-27   1.00000000e+00 ...,   5.62195972e-31\n",
      "    4.12873601e-25   7.36956905e-32]\n",
      " [  1.00000000e+00   2.68017627e-36   2.72572069e-27 ...,   6.51895031e-31\n",
      "    2.33977885e-36   8.00238096e-29]\n",
      " [  1.37983586e-22   1.42332091e-23   5.53744590e-21 ...,   3.08784901e-17\n",
      "    1.50148702e-15   1.00000000e+00]\n",
      " ..., \n",
      " [  4.38490945e-29   1.03504554e-25   1.07896807e-28 ...,   7.30065366e-24\n",
      "    5.03217401e-23   3.08675592e-23]\n",
      " [  1.24491418e-15   7.59870858e-20   7.00199406e-20 ...,   1.49559564e-12\n",
      "    6.63356711e-16   1.00000000e+00]\n",
      " [  2.07238681e-33   1.24711329e-25   1.00000000e+00 ...,   5.49375631e-32\n",
      "    1.38053459e-24   6.86329642e-33]]\n",
      "[2 0 9 ..., 3 9 2]\n",
      "saved predictions to a CSV file\n"
     ]
    }
   ],
   "source": [
    "#predict the classes for the test dataset\n",
    "test = pd.read_csv(\"/home/mayur/Deep_learning_and_Project/tensorflow/handwritten_digits/test.csv\").values\n",
    "test = test.reshape(test.shape[0], 28, 28, 1).astype('float32')\n",
    "test = test/255.0\n",
    "\n",
    "test_pred_prob = model.predict(test)\n",
    "print(test_pred_prob)\n",
    "\n",
    "test_pred = np.argmax(test_pred_prob, axis = 1)\n",
    "print(test_pred)\n",
    "\n",
    "if not os.path.exists('/home/mayur/Deep_learning_and_Project/tensorflow/handwritten_digits/results'):\n",
    "    os.makedirs('/home/mayur/Deep_learning_and_Project/tensorflow/handwritten_digits/results')\n",
    "    np.savetxt('/home/mayur/Deep_learning_and_Project/tensorflow/handwritten_digits/results/predictions.csv', np.c_[range(1, len(test_pred) + 1), test_pred], delimiter = ',', header = 'ImageId,Label', comments = '', fmt = '%d')\n",
    "    print(\"saved predictions to a CSV file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
